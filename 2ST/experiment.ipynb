{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计机器学习第二次作业\n",
    "### 葡萄酒品种分类——基于Pytorch\n",
    "王恒 计算数学 220220934161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d00ee313b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import time\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data =  datasets.load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alcohol',\n",
       " 'malic_acid',\n",
       " 'ash',\n",
       " 'alcalinity_of_ash',\n",
       " 'magnesium',\n",
       " 'total_phenols',\n",
       " 'flavanoids',\n",
       " 'nonflavanoid_phenols',\n",
       " 'proanthocyanins',\n",
       " 'color_intensity',\n",
       " 'hue',\n",
       " 'od280/od315_of_diluted_wines',\n",
       " 'proline']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = torch.tensor(data.data)\n",
    "data_labels = torch.tensor(data.target)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([178, 13])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([178, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = torch.cat([data_features,data_labels],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = data_set.shape[0]\n",
    "alpha = 0.7\n",
    "train_len = int(length*alpha)\n",
    "test_len = length - train_len\n",
    "train_data, test_data = torch.utils.data.random_split(data_set,[train_len,test_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor([item.numpy() for item in train_data])\n",
    "test_data = torch.tensor([item.numpy() for item in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([124, 14])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54, 14])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class NaiveBayes(nn.Module):\n",
    "    def __init__(self, n, full_labels, S, lamb) -> None:\n",
    "        super(NaiveBayes,self).__init__()\n",
    "        # 归一化参数\n",
    "        self.max = None\n",
    "        self.min = None\n",
    "        self.n = n # 特征数量\n",
    "        self.full_labels = full_labels # 所有标签\n",
    "        self.K = len(full_labels) # 标签数量\n",
    "        self.lamb = lamb # 贝叶斯估计参数lambda\n",
    "        self.S = S # 每个特征分划区间数，这里默认都为S\n",
    "        self.cond_prob = torch.zeros([self.K,self.n,S]) # 条件概率\n",
    "        self.pre_prob = torch.zeros([self.K]) # 先验概率\n",
    "\n",
    "    def forward(self, features):\n",
    "        B,n = features.shape\n",
    "        assert n == self.n\n",
    "        post_prob = torch.ones([B,1])*self.pre_prob\n",
    "        # 归一化\n",
    "        features = (features - self.min) / (self.max - self.min) * 2 - 1\n",
    "        delta_x = 2 / (self.S - 2)\n",
    "        for i in range(B):\n",
    "            for j in range(self.K):\n",
    "                for k in range(n):\n",
    "                    if features[i,k] < -1: post_prob[i,j] *= self.cond_prob[j,k,0]\n",
    "                    elif features[i,k] >= 1: post_prob[i,j] *= self.cond_prob[j,k,-1]\n",
    "                    else:\n",
    "                        for h in range(self.S-2):\n",
    "                            l = -1 + h * delta_x\n",
    "                            r = l + delta_x\n",
    "                            if features[i,k] >= l and features[i,k] < r:\n",
    "                                post_prob[i,j] *= self.cond_prob[j,k,h+1]\n",
    "                                break\n",
    "        return self.full_labels[torch.argmax(post_prob,dim=1)]\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        # 计算先验概率\n",
    "        N,_ = train_data.shape\n",
    "        self.max = torch.max(train_data[:,:-1],dim=0).values\n",
    "        self.min = torch.min(train_data[:,:-1],dim=0).values\n",
    "        train_data[:,:-1] = (train_data[:,:-1] - self.min) / (self.max - self.min) * 2 - 1\n",
    "        features = train_data[:,:-1] # 特征\n",
    "        labels = train_data[:,-1:].int() # 标签\n",
    "        delta_x = 2 / (self.S - 2)\n",
    "        for i in range(self.K):\n",
    "            labels_i = labels == self.full_labels[i]\n",
    "            self.pre_prob[i] = (labels_i.sum() + self.lamb) / (N + self.K*self.lamb)\n",
    "            for j in range(self.n):\n",
    "                self.cond_prob[i,j,0] = 1 / self.S\n",
    "                for k in range(self.S-1):\n",
    "                    l = -1 + k * delta_x\n",
    "                    r = l + delta_x\n",
    "                    features_ij = features[labels_i[:,0],j]\n",
    "                    features_ijk = features_ij[(features_ij>=l)*(features_ij<r)]\n",
    "                    self.cond_prob[i,j,k+1] = (features_ijk.shape[0] + self.lamb) / (labels_i.sum() + self.S*self.lamb)\n",
    "        return self.pre_prob, self.cond_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayse = NaiveBayes(\n",
    "    n=train_data.shape[1]-1,\n",
    "    full_labels=torch.tensor([0,1,2]),\n",
    "    S=10,\n",
    "    lamb=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "_,_ = naive_bayse.fit(\n",
    "    train_data = copy.deepcopy(train_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "predicts = naive_bayse(\n",
    "    copy.deepcopy(test_data[:,:-1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 2, 1, 2, 0, 1, 1, 1, 1, 0, 1, 0, 2, 0, 1, 1, 1, 2, 2, 1, 2, 0, 0,\n",
       "        1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 2, 1, 1, 2, 1,\n",
       "        0, 1, 2, 2, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 2, 1, 2, 0, 1, 2, 1, 1, 0, 1, 0, 2, 0, 1, 1, 1, 2, 2, 1, 2, 0, 0,\n",
       "        1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1,\n",
       "        0, 1, 2, 2, 0, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:,-1].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9630)\n"
     ]
    }
   ],
   "source": [
    "# 打印准确率\n",
    "print(torch.sum(predicts == test_data[:,-1].int()) / test_data.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义模型（ID3算法）\n",
    "# class Node():\n",
    "#     def __init__(self, father = None, childrens = [], mark = None) -> None:\n",
    "#         super(Node,self).__init__()\n",
    "#         self.father = father\n",
    "#         self.childrens = childrens\n",
    "#         self.mark = mark\n",
    "#         self.split = []\n",
    "    \n",
    "#     def is_leaf(self):\n",
    "#         if self.mark is None:\n",
    "#             return False\n",
    "#         else:\n",
    "#             return True\n",
    "\n",
    "# class DecisionTree(nn.Module):\n",
    "#     def __init__(self, full_features, full_labels, S, esplison = 0.001) -> None:\n",
    "#         super(DecisionTree,self).__init__()\n",
    "#         self.esplison = esplison\n",
    "#         self.S = S # 特征划分数\n",
    "#         self.full_features = full_features\n",
    "#         self.feature_num = len(full_features)\n",
    "#         self.full_labels = full_labels\n",
    "#         self.K = len(full_labels)\n",
    "#         # 归一化参数\n",
    "#         self.min = None\n",
    "#         self.max = None\n",
    "#         self.Tree = Node()\n",
    "\n",
    "#     def create_tree(self,D,A,node = None,root = None):\n",
    "#         if node is None: \n",
    "#             node = self.Tree\n",
    "#             root = self.Tree\n",
    "#             # 归一化\n",
    "#             self.max = torch.max(D[:,:-1],dim=0).values\n",
    "#             self.min = torch.min(D[:,:-1],dim=0).values\n",
    "#             D[:,:-1] = (D[:,:-1] - self.min) / (self.max - self.min) * 2 - 1\n",
    "#         features = D[:,:-1]\n",
    "#         labels = D[:,-1:]\n",
    "#         # 只有一种标签时候直接返回\n",
    "#         if torch.unique(labels).shape[0] == 1:\n",
    "#             node.mark = D[0,-1]\n",
    "#             return root\n",
    "#         # 标签数量为空时直接返回\n",
    "#         elif len(A) == 0:\n",
    "#             count = torch.bincount(labels)\n",
    "#             mark = torch.argmax(count)\n",
    "#             node.mark = mark\n",
    "#             return root\n",
    "#         else:\n",
    "#             # 计算各特征的信息增益，选择最大信息增益特征\n",
    "#             g = self.information_gain(D,A[0])\n",
    "#             Ag = 0\n",
    "#             for i in range(len(A) - 1):\n",
    "#                 gi = self.information_gain(D,A[i+1])\n",
    "#                 if gi > g:\n",
    "#                     g = gi\n",
    "#                     Ag = i+1\n",
    "#             # 达到精度返回\n",
    "#             if g < self.esplison:\n",
    "#                 count = torch.bincount(labels)\n",
    "#                 mark = torch.argmax(count)\n",
    "#                 node.mark = mark\n",
    "#                 return root\n",
    "#             # 未达到精度继续递归生成子节点\n",
    "#             else:\n",
    "#                 l = None\n",
    "#                 r = -1\n",
    "#                 delta_x = 2 / (self.S - 2)\n",
    "#                 feature_i = features[:,Ag]\n",
    "#                 Ai = copy.deepcopy(A)\n",
    "#                 Ai[Ag] = False\n",
    "#                 for i in range(self.S):\n",
    "#                     if i == self.S - 1: r = None\n",
    "#                     if l is None:\n",
    "#                         index = feature_i < r\n",
    "#                         # 当前数据不空\n",
    "#                         if index.sum() > 0:\n",
    "#                             node.split.append(lambda x: x < r) # 用于判别并分划当前特征\n",
    "#                             Di = D[index,:] # Di一定非空\n",
    "#                             assert Di.shape[0] > 0\n",
    "#                             nodei = Node(father=node)\n",
    "#                             # 递归处理子节点\n",
    "#                             self.create_tree(Di,Ai,nodei,root)\n",
    "#                             node.childrens.append(nodei)\n",
    "#                             l = r\n",
    "#                             r = r + delta_x\n",
    "#                     elif r is None:\n",
    "#                         index = feature_i >= l\n",
    "#                         # 当前数据不空\n",
    "#                         if index.sum() > 0:\n",
    "#                             node.split.append(lambda x: x < r) # 用于判别并分划当前特征\n",
    "#                             Di = D[index,:] # Di一定非空\n",
    "#                             assert Di.shape[0] > 0\n",
    "#                             nodei = Node(father=node)\n",
    "#                             # 递归处理子节点\n",
    "#                             self.create_tree(Di,Ai,nodei,root)\n",
    "#                             node.childrens.append(nodei)\n",
    "#                             l = r\n",
    "#                             r = r + delta_x\n",
    "                        \n",
    "                            \n",
    "                \n",
    "\n",
    "#     # 计算经验熵\n",
    "#     def empirical_entropy(self,D):\n",
    "#         labels = data_set[:,-1:]\n",
    "#         HD = 0\n",
    "#         for j in range(self.K):\n",
    "#             labels_j = labels == self.full_labels[j]\n",
    "#             HD -= labels_j.sum() / D * torch.log2(labels_j.sum() / D)\n",
    "#         return HD\n",
    "\n",
    "#     # 计算A对于数据集D的经验条件熵\n",
    "#     def empirical_cond_entropy(self,D,A):\n",
    "#         D_ = D.shape[0]\n",
    "#         index = self.full_features == A\n",
    "#         features = D[:,:-1]\n",
    "#         labels = D[:,-1:]\n",
    "#         DA = features[:,index]\n",
    "#         DAi = DA[DA < -1,None]\n",
    "#         Li = labels[DA < -1,:]\n",
    "#         HDA = len(DAi) / D_ * self.empirical_entropy(torch.cat([DAi,Li],dim=1))\n",
    "#         DAi = DA[DA >= 1,None]\n",
    "#         Li = labels[DA >= 1,:]\n",
    "#         HDA += len(DAi) / D_ * self.empirical_entropy(torch.cat([DAi,Li],dim=1))\n",
    "#         delta_x = 2 / (self.S - 2)\n",
    "#         for i in range(self.S - 2):\n",
    "#             l = -1 + i * delta_x\n",
    "#             r = l + delta_x\n",
    "#             index_i = (DA >= l) * (DA < r)\n",
    "#             DAi = DA[index_i,None]\n",
    "#             Li = labels[index_i,:]\n",
    "#             HDA += len(DAi) / D_ * self.empirical_entropy(torch.cat([DAi,Li],dim=1))\n",
    "#         return HDA\n",
    "\n",
    "#     # 计算信息增益\n",
    "#     def information_gain(self,data_set,feature):\n",
    "#         if self.min is None or self.max is None:\n",
    "#             self.max = torch.max(train_data[:,:-1],dim=0).values\n",
    "#             self.min = torch.min(train_data[:,:-1],dim=0).values\n",
    "#         # 特征归一化到[-1,1]\n",
    "#         train_data[:,:-1] = (train_data[:,:-1] - self.min) / (self.max - self.min) * 2 - 1\n",
    "\n",
    "#         index = self.full_features == feature\n",
    "#         D,_ = data_set\n",
    "#         assert torch.sum(index) == 1 and D > 1\n",
    "#         # 计算经验熵\n",
    "#         HD = self.empirical_entropy(data_set)\n",
    "#         # 计算feature对于数据集data_set的经验条件熵\n",
    "#         HDA = self.empirical_cond_entropy(data_set,feature)\n",
    "#         return HD - HDA\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic(nn.Module):\n",
    "    def __init__(self,feature_num,class_num) -> None:\n",
    "        super(Logistic,self).__init__()\n",
    "        self.feature_num = feature_num\n",
    "        self.class_num = class_num\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.BatchNorm1d(feature_num),\n",
    "            nn.Linear(feature_num,class_num-1,bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,d = x.shape\n",
    "        assert d == self.feature_num and B > 0\n",
    "        y = torch.cat([torch.exp(self.linear(x)),torch.ones([B,1])],dim=1)\n",
    "        y = y / torch.sum(y,dim=1,keepdim=True)\n",
    "        return y\n",
    "    \n",
    "# 负对数似然损失函数\n",
    "class NLLLoss(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,y_pre,y_rel):\n",
    "        assert y_pre.shape == y_rel.shape\n",
    "        loss = -torch.log(y_pre)*y_rel\n",
    "        return torch.sum(loss)\n",
    "\n",
    "def train(model, data_set, batch_size, epoch = 1000, learning_rate = 1e-3):\n",
    "    N,_ = data_set.shape\n",
    "    criterion = NLLLoss()\n",
    "    optim = torch.optim.SGD(model.parameters(),learning_rate,momentum=0) # 动量设置为0\n",
    "    \n",
    "    start = time.time()\n",
    "    loss_values = torch.zeros(epoch)\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        optim.zero_grad()\n",
    "        index = torch.randint(0,N,[batch_size]) # 随机选取batch条数据\n",
    "        data_i = data_set[index,:]\n",
    "        x_i = data_i[:,:-1].to(torch.float32)\n",
    "        y_i = nn.functional.one_hot(data_i[:,-1].to(torch.int64),num_classes=model.class_num)\n",
    "        outputs = model(x_i)\n",
    "        loss = criterion(outputs,y_i)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        model.eval()\n",
    "        loss_values[i] = loss.item()\n",
    "\n",
    "        print('\\r%5d/{}|{}{}|{:.2f}s  [Loss: %e]'.format(\n",
    "            epoch,\n",
    "            \"#\"*int((i+1)/epoch*50),\n",
    "            \" \"*(50-int((i+1)/epoch*50)),\n",
    "            time.time() - start) %\n",
    "            (i+1,\n",
    "            loss_values[i]), end = ' ', flush=True)\n",
    "    print(\"\\nTraining has been completed.\")\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Logistic(\n",
    "    feature_num=train_data.shape[1]-1,\n",
    "    class_num=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/1000|##################################################|3.12s  [Loss: 4.260702e+00]               \n",
      "Training has been completed.\n"
     ]
    }
   ],
   "source": [
    "loss_values = train(\n",
    "    model=model,\n",
    "    data_set=train_data,\n",
    "    batch_size=10,\n",
    "    epoch=1000,\n",
    "    learning_rate=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model(\n",
    "    test_data[:,:-1].float()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = torch.argmax(predicts,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 2, 1, 2, 0, 1, 2, 1, 1, 0, 1, 0, 2, 0, 1, 1, 1, 2, 2, 1, 2, 0, 0,\n",
       "        1, 0, 0, 0, 2, 1, 1, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1,\n",
       "        0, 1, 2, 2, 0, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 2, 1, 2, 0, 1, 2, 1, 1, 0, 1, 0, 2, 0, 1, 1, 1, 2, 2, 1, 2, 0, 0,\n",
       "        1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1,\n",
       "        0, 1, 2, 2, 0, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:,-1].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9815)\n"
     ]
    }
   ],
   "source": [
    "# 打印准确率\n",
    "print(torch.sum(predicts == test_data[:,-1].int()) / test_data.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class SVMLinear(nn.Module):\n",
    "    def __init__(self,feature_num) -> None:\n",
    "        super(SVMLinear,self).__init__()\n",
    "        self.feature_num = feature_num\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.BatchNorm1d(feature_num), # 特征批量归一化，学习归一化参数\n",
    "            nn.Linear(feature_num,1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,d = x.shape\n",
    "        assert B > 0 and d == self.feature_num\n",
    "        return torch.sign(self.linear(x))\n",
    "\n",
    "# 合页损失函数\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self,lamb) -> None:\n",
    "        super(HingeLoss,self).__init__()\n",
    "        self.lamb = lamb\n",
    "\n",
    "    def forward(self,res_pre_linear_values,res_rel,parameters):\n",
    "        return torch.sum(torch.relu(1 - res_rel*res_pre_linear_values)) + self.lamb*torch.norm(parameters)**2\n",
    "    \n",
    "def train_svm(model, data_set, batch_size, lamb=1, epoch = 1000, learning_rate = 1e-3):\n",
    "    N,_ = data_set.shape\n",
    "    criterion = HingeLoss(lamb)\n",
    "    optim = torch.optim.SGD(model.parameters(),learning_rate,momentum=0) # 动量设置为0\n",
    "    \n",
    "    start = time.time()\n",
    "    loss_values = torch.zeros(epoch)\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        optim.zero_grad()\n",
    "        index = torch.randint(0,N,[batch_size]) # 随机选取batch条数据\n",
    "        data_i = data_set[index,:]\n",
    "        x_i = data_i[:,:-1].to(torch.float32)\n",
    "        y_i = data_i[:,-1:].to(torch.int32)\n",
    "        outputs = model.linear(x_i)\n",
    "        loss = criterion(outputs,y_i,model.linear[1].weight)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        model.eval()\n",
    "        loss_values[i] = loss.item()\n",
    "\n",
    "        print('\\r%5d/{}|{}{}|{:.2f}s  [Loss: %e]'.format(\n",
    "            epoch,\n",
    "            \"#\"*int((i+1)/epoch*50),\n",
    "            \" \"*(50-int((i+1)/epoch*50)),\n",
    "            time.time() - start) %\n",
    "            (i+1,\n",
    "            loss_values[i]), end = ' ', flush=True)\n",
    "    print(\"\\nTraining has been completed.\")\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1 = SVMLinear(\n",
    "    feature_num=train_data.shape[1]-1\n",
    ")\n",
    "svm2 = SVMLinear(\n",
    "    feature_num=train_data.shape[1]-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_svm1 = copy.deepcopy(train_data)\n",
    "train_data_svm2 = copy.deepcopy(train_data)\n",
    "train_data_svm1[train_data_svm1[:,-1].int() > 0,-1] = -1\n",
    "train_data_svm1[train_data_svm1[:,-1].int() == 0,-1] = 1\n",
    "train_data_svm2 = train_data_svm2[train_data_svm2[:,-1].int() > 0,:]\n",
    "train_data_svm2[train_data_svm2[:,-1].int() == 2,-1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([124, 14])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_svm1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([83, 14])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_svm2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,\n",
       "         1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,\n",
       "        -1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
       "        -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
       "        -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,\n",
       "         1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.,\n",
       "        -1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
       "         1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
       "         1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1., -1.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_svm1[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,\n",
       "        -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,\n",
       "         1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
       "         1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,\n",
       "        -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_svm2[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/1000|##################################################|3.29s  [Loss: 1.100088e+00]        \n",
      "Training has been completed.\n"
     ]
    }
   ],
   "source": [
    "loss_values = train_svm(\n",
    "    model=svm1,\n",
    "    data_set=train_data_svm1,\n",
    "    batch_size=10,\n",
    "    lamb=1,\n",
    "    epoch=1000,\n",
    "    learning_rate=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/1000|##################################################|3.07s  [Loss: 2.465142e+00]         \n",
      "Training has been completed.\n"
     ]
    }
   ],
   "source": [
    "loss_values = train_svm(\n",
    "    model=svm2,\n",
    "    data_set=train_data_svm2,\n",
    "    batch_size=10,\n",
    "    lamb=1,\n",
    "    epoch=1000,\n",
    "    learning_rate=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_svm1 = svm1(\n",
    "    test_data[:,:-1].to(torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_svm2 = svm2(\n",
    "    test_data[pre_svm1[:,0] < 0,:-1].to(torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = pre_svm1[:,0]\n",
    "k = 0\n",
    "for i in range(predicts.shape[0]):\n",
    "    if predicts[i] > 0: predicts[i] = 0\n",
    "    elif predicts[i] < 0 and pre_svm2[k,0] > 0:\n",
    "        predicts[i] = 1\n",
    "        k += 1\n",
    "    else:\n",
    "        predicts[i] = 2\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 2, 2, 1, 2, 0, 0, 2, 0, 1, 1,\n",
       "        2, 0, 1, 1, 1, 0, 2, 1, 2, 2, 0, 2, 2, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 1,\n",
       "        1, 2, 0, 1, 0, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 2, 2, 1, 2, 0, 0, 2, 1, 1, 1,\n",
       "        2, 0, 1, 1, 1, 0, 2, 1, 2, 2, 0, 2, 2, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 1,\n",
       "        1, 2, 0, 1, 0, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:,-1].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9630)\n"
     ]
    }
   ],
   "source": [
    "# 打印准确率\n",
    "print(torch.sum(predicts == test_data[:,-1].int()) / test_data.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非线性"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层非线性感知机（神经网络）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,feature_num,class_num,hidden_dim=20,layer_num=2) -> None:\n",
    "        super(MLP,self).__init__()\n",
    "        self.feature_num = feature_num\n",
    "        self.class_num = class_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_num = layer_num\n",
    "        self.model = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [(\"input_layer\",\n",
    "                    nn.Sequential(\n",
    "                        nn.BatchNorm1d(feature_num),\n",
    "                        nn.Linear(feature_num,hidden_dim),\n",
    "                        nn.Tanh()\n",
    "                    ))] + \n",
    "                [(\"hidden_layer_\"+str(i+1),\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(hidden_dim,hidden_dim),\n",
    "                        nn.Tanh()\n",
    "                    )) for i in range(layer_num-1)] + \n",
    "                [(\"output_layer\",\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(hidden_dim,class_num),\n",
    "                        nn.Softmax(dim=1)\n",
    "                    ))]\n",
    "            )\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(\n",
    "    feature_num=train_data.shape[1]-1,\n",
    "    class_num=3,\n",
    "    hidden_dim=20,\n",
    "    layer_num=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/1000|##################################################|3.81s  [Loss: 1.896516e+00]              \n",
      "Training has been completed.\n"
     ]
    }
   ],
   "source": [
    "loss_values = train(\n",
    "    model=mlp,\n",
    "    data_set=train_data,\n",
    "    batch_size=10,\n",
    "    epoch=1000,\n",
    "    learning_rate=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = mlp(\n",
    "    test_data[:,:-1].float()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = torch.argmax(predicts,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 2, 1, 2, 0, 1, 2, 1, 1, 0, 1, 0, 2, 0, 1, 1, 1, 2, 2, 1, 2, 0, 0,\n",
       "        1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1,\n",
       "        0, 1, 2, 2, 0, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 2, 1, 2, 0, 1, 2, 1, 1, 0, 1, 0, 2, 0, 1, 1, 1, 2, 2, 1, 2, 0, 0,\n",
       "        1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1,\n",
       "        0, 1, 2, 2, 0, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:,-1].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 打印准确率\n",
    "print(torch.sum(predicts == test_data[:,-1].int()) / test_data.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
